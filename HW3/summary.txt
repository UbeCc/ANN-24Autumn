########################
# Additional Files
########################
# run.sh
# sample_sentence.py
# output.txt
# data
# tokenizer
# wandb

########################
# Filled Code
########################
# ../codes/main.py:1
            tgt_ids = input_ids.clone()
            # logits: (bsize, seq_len, vocab_size)
            shift_logits = lm_logits[..., :-1, :].contiguous()
            shift_labels = tgt_ids[..., 1:].contiguous() # predict from the 2nd position
            position_mask = torch.ones_like(tgt_ids)
            position_mask[:, 1:] = (shift_labels != PAD_ID)
            position_mask =  position_mask.float()[:, :-1].contiguous()
            loss = (ce_loss_fct(shift_logits.transpose(1, 2), shift_labels) * position_mask).sum(dim=1) / (position_mask.sum(dim=1) + 1e-6)

# ../codes/model_tfmr.py:1
            torch.tril(torch.ones((max_positions, max_positions), dtype=torch.uint8).view(
                1, 1, max_positions, max_positions
            ))

# ../codes/model_tfmr.py:2
        # query: (batch_size, num_heads, query_seq_length, head_dim)
        # key: (batch_size, num_heads, key_seq_length, head_dim)
        # value: (batch_size, num_heads, value_seq_length, head_dim)
        # key & value share the same length

        # debug(f'q: {query.shape}, k: {key.shape}')
        attn_weights = query @ key.transpose(-1, -2)
        causal_mask = self.bias[..., key.size(-2) - query.size(-2): key.size(-2), :key.size(-2)]
        attn_weights = attn_weights * causal_mask + self.masked_bias.to(attn_weights.device).to(attn_weights.dtype) * (1 - causal_mask)
        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
        attn_output = attn_weights @ value

# ../codes/model_tfmr.py:3
        assert tensor.size(-1) == num_heads * attn_head_size, f"hidden_size: {tensor.size(-1)} != num_heads * attn_head_size: {num_heads * attn_head_size}"
        x_shape = tensor.size()[: -1] + (num_heads, attn_head_size)
        return tensor.view(*x_shape).permute(0, 2, 1, 3)

# ../codes/model_tfmr.py:4
        tensor = tensor.permute(0, 2, 1, 3).contiguous()
        x_shape = tensor.size()[:-2] + (num_heads * attn_head_size,)
        return tensor.view(*x_shape)

# ../codes/model_tfmr.py:5
        hidden_states = attn_output + residual

        if NORM == "pre":
            residual = hidden_states
            hidden_states = self.ln_2(hidden_states)
            feed_forward_hidden_states = self.mlp(hidden_states)
            # residual connection
            hidden_states = residual + feed_forward_hidden_states
        elif NORM == "post":
            residual = hidden_states
            # residual connection
            hidden_states = self.ln_2(residual + self.mlp(hidden_states))
        elif NORM == "rms":
            residual = hidden_states
            hidden_states = self.ln_2(hidden_states)
            feed_forward_hidden_states = self.mlp(hidden_states)
            hidden_states = residual + feed_forward_hidden_states
        else:
            raise NotImplementedError
        position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.int, device=device)
        position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])
        position_embeds = self.wpe(position_ids)

# ../codes/model_tfmr.py:6

            # Reference: https://discuss.huggingface.co/t/gpt-2-shift-logits-and-labels/812
            shift_logits = lm_logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            position_mask = torch.ones_like(labels)
            position_mask[:, 1:] = (shift_labels != PAD_ID)
            position_mask = position_mask.float()[:, :-1].contiguous()
            loss = (ce_loss_fct(shift_logits.transpose(1, 2), shift_labels) * position_mask).sum(dim=1) \
                / (position_mask.sum(dim=1) + 1e-5)
            loss = loss.mean()

            # padding_mask = shift_labels.eq(PAD_ID)
            # if padding_mask.any():
            #     loss = loss.view(shift_labels.shape)
            #     loss.masked_fill_(padding_mask, 0.0)
            #     loss = loss.sum() / (~padding_mask).sum()
            # else:
            #     loss = loss.mean()

# ../codes/model_tfmr.py:7
                        # shape of logits is (batch_size, num_vocabs)
                        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
                        cumu_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                        unused_indices = cumu_probs > top_p
                        unused_indices[:, 1:] = unused_indices[:, :-1].clone() #
                        unused_indices[:, 0] = 0
                        unused_indices = unused_indices.scatter(1, sorted_indices, unused_indices)
                        logits[unused_indices] = float('-inf')


########################
# References
########################
# https://discuss.huggingface.co/t/gpt-2-shift-logits-and-labels/812

########################
# Other Modifications
########################
# _codes/main.py -> ../codes/main.py
# 1 + import wandb
# 18 + USE_WANDB = True
# 19 +
# 22 + parser.add_argument("--runid", type=str, default="run",
# 23 +     help="Run ID for Weights and Biases. Default: run")
# 39 - parser.add_argument("--pretrain_dir", type=str, default="None",
# 39 ?                                                         -    -
# 44 + parser.add_argument("--pretrain_dir", type=str, default=None,
# 196 +
# 197 +     if USE_WANDB:
# 198 +         wandb.init(
# 199 +             project="ANN-HW3",
# 200 +             name=args.runid
# 201 +         )
# 202 +
# 205 +
# 250 +                     if USE_WANDB:
# 251 +                         wandb.log({"train_loss": np.mean(losses[-10:]), "epoch": epoch, "batch": batch_num})
# 256 +             if USE_WANDB:
# 257 +                 wandb.log({"epoch": epoch, "train_loss": train_loss, "val_loss": val_loss, "val_ppl": val_ppl})
# 258 +
# 274 +                 if USE_WANDB:
# 275 +                     wandb.log({"best_val_ppl": best_val_ppl, "best_epoch": best_epoch})
# 286 +         if USE_WANDB:
# 287 +             wandb.log({"test_loss": test_loss, "test_ppl": test_ppl})
# 288 +
# 266 -         with open(f"output_{args.decode_strategy}.txt", "w") as fout:
# 291 +         with open(f"output_{args.runid}_{args.decode_strategy}.txt", "w") as fout:
# 291 ?                           +++++++++++++
# 299 +         if USE_WANDB:
# 300 +             wandb.log(eval_result)
# 302 +     if USE_WANDB:
# 303 +         wandb.finish()
# _codes/tokenizer.py -> ../codes/tokenizer.py
# 124 -     )
# 124 +     )
# 124 ?      +
# 125 +
# 126 + if __name__ == "__main__":
# 127 +     import os
# 128 +     models_dir = 'tokenizer'
# 129 +     tokenizer = get_tokenizer(models_dir)
# 130 +     text = "This is a test sentence for BPE tokenizer."
# 131 +     tokens = tokenizer.tokenize(text)
# 132 +     print("Raw Text:", text)
# 133 +     print("BPE Tokenized Text:", tokens)
# 134 +     encoded_tokens = tokenizer.encode(text)
# 135 +     print("BPE Encoded Tokens:", encoded_tokens)
# 136 +     decoded_text = tokenizer.decode(encoded_tokens)
# 137 +     print("Decoded Text:", decoded_text)
# _codes/model_tfmr.py -> ../codes/model_tfmr.py
# 1 + import math
# 7 +
# 8 + NORM = "pre"
# 9 + # NORM = "post"
# 10 + # NORM = "rms"
# 11 +
# 12 + DEBUG = True
# 13 +
# 14 + def debug(text):
# 15 +     if DEBUG:
# 16 +         print(text)
# 26 + class RMSNorm(nn.Module):
# 27 +     def __init__(self, hidden_size, eps=1e-6):
# 28 +         super().__init__()
# 29 +         self.weight = nn.Parameter(torch.ones(hidden_size))
# 30 +         self.eps = eps
# 31 +
# 32 +     def forward(self, hidden_states):
# 33 +         variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)
# 34 +         hidden_states = hidden_states * torch.rsqrt(variance + self.eps)
# 35 +         return self.weight * hidden_states
# 36 +
# 162 + class GQAttention(nn.Module):
# 163 +     def __init__(self, config):
# 164 +         super().__init__()
# 165 +
# 166 +         max_positions = config.max_position_embeddings
# 167 +         self.register_buffer(
# 168 +             "bias",
# 169 +             torch.tril(torch.ones((max_positions, max_positions), dtype=torch.uint8).view(
# 170 +                 1, 1, max_positions, max_positions
# 171 +             ))
# 172 +         )
# 173 +         self.register_buffer("masked_bias", torch.tensor(-1e4))
# 174 +
# 175 +         config.num_key_value_groups = config.num_attention_heads // 4
# 176 +         self.embed_dim = config.hidden_size
# 177 +         self.num_heads = config.num_attention_heads
# 178 +         self.head_dim = self.embed_dim // self.num_heads
# 179 +         self.num_groups = config.num_key_value_groups
# 180 +         self.num_kv_heads = self.num_heads // self.num_groups
# 181 +
# 182 +         if self.head_dim * self.num_heads != self.embed_dim:
# 183 +             raise ValueError(
# 184 +                 f"`embed_dim` must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`: {self.num_heads})."
# 185 +             )
# 186 +
# 187 +         self.scale_attn_weights = config.scale_attn_weights
# 188 +         self.q_proj = TransposeLinear(self.embed_dim, self.embed_dim)
# 189 +         self.k_proj = TransposeLinear(self.embed_dim // self.num_groups, self.embed_dim)
# 190 +         self.v_proj = TransposeLinear(self.embed_dim // self.num_groups, self.embed_dim)
# 191 +         self.o_proj = TransposeLinear(self.embed_dim, self.embed_dim)
# 192 +
# 193 +         self.attn_dropout = nn.Dropout(config.attn_pdrop)
# 194 +         self.resid_dropout = nn.Dropout(config.resid_pdrop)
# 195 +
# 196 +     def _attn(self, query, key, value):
# 197 +         attn_weights = query @ key.transpose(-1, -2)
# 198 +
# 199 +         if self.scale_attn_weights:
# 200 +             attn_weights = attn_weights / (float(value.size(-1)) ** 0.5)
# 201 +
# 202 +         causal_mask = self.bias[..., key.size(-2) - query.size(-2): key.size(-2), :key.size(-2)]
# 203 +         attn_weights = attn_weights * causal_mask + self.masked_bias.to(attn_weights.device).to(attn_weights.dtype) * (1 - causal_mask)
# 204 +
# 205 +         attn_weights = nn.functional.softmax(attn_weights, dim=-1)
# 206 +         attn_weights = self.attn_dropout(attn_weights)
# 207 +         attn_output = attn_weights @ value
# 208 +
# 209 +         return attn_output, attn_weights
# 210 +
# 211 +     def _split_heads(self, tensor, num_heads, attn_head_size):
# 212 +         assert tensor.size(-1) == num_heads * attn_head_size, f"hidden_size: {tensor.size(-1)} != num_heads * attn_head_size: {num_heads * attn_head_size}"
# 213 +         x_shape = tensor.size()[: -1] + (num_heads, attn_head_size)
# 214 +         return tensor.view(*x_shape).permute(0, 2, 1, 3)
# 215 +
# 216 +     def _merge_heads(self, tensor, num_heads, attn_head_size):
# 217 +         tensor = tensor.permute(0, 2, 1, 3).contiguous()
# 218 +         x_shape = tensor.size()[:-2] + (num_heads * attn_head_size,)
# 219 +         return tensor.view(*x_shape)
# 220 +
# 221 +
# 222 +     def forward(
# 223 +         self,
# 224 +         hidden_states,
# 225 +         layer_past=None,
# 226 +         use_cache=False,
# 227 +     ):
# 228 +         query = self.q_proj(hidden_states)
# 229 +         key = self.k_proj(hidden_states)
# 230 +         value = self.v_proj(hidden_states)
# 231 +
# 232 +         query = self._split_heads(query, self.num_heads, self.head_dim)
# 233 +         key = self._split_heads(key, self.num_kv_heads, self.head_dim)
# 234 +         value = self._split_heads(value, self.num_kv_heads, self.head_dim)
# 235 +
# 236 +         # Repeat k and v for each group
# 237 +         key = key.repeat_interleave(self.num_groups, dim=1)
# 238 +         value = value.repeat_interleave(self.num_groups, dim=1)
# 239 +
# 240 +         if layer_past is not None:
# 241 +             past_key, past_value = layer_past
# 242 +             key = torch.cat((past_key, key), dim=-2)
# 243 +             value = torch.cat((past_value, value), dim=-2)
# 244 +
# 245 +         if use_cache is True:
# 246 +             present = (key, value)
# 247 +         else:
# 248 +             present = None
# 249 +
# 250 +         attn_output, attn_weights = self._attn(query, key, value)
# 251 +
# 252 +         attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)
# 253 +         attn_output = self.o_proj(attn_output)
# 254 +         attn_output = self.resid_dropout(attn_output)
# 255 +
# 256 +         outputs = (attn_output, present)
# 257 +         outputs += (attn_weights,)
# 258 +
# 259 +         return outputs  # a, present, (attentions)
# 149 -         self.ln_1 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
# 284 +         self.ln_1 = RMSNorm(hidden_size, eps=config.layer_norm_epsilon) if NORM == "rms" else nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
# 150 -         self.attn = TfmrAttention(config)
# 285 +         # self.attn = TfmrAttention(config)
# 285 ?        ++
# 151 -         self.ln_2 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
# 286 +         self.attn = GQAttention(config)
# 287 +         self.ln_2 = RMSNorm(hidden_size, eps=config.layer_norm_epsilon) if NORM == "rms" else nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
# 308 +         hidden_states = attn_output + residual
# 309 +
# 310 +         if NORM == "pre":
# 311 +             residual = hidden_states
# 312 +             hidden_states = self.ln_2(hidden_states)
# 313 +             feed_forward_hidden_states = self.mlp(hidden_states)
# 314 +             # residual connection
# 315 +             hidden_states = residual + feed_forward_hidden_states
# 316 +         elif NORM == "post":
# 317 +             residual = hidden_states
# 318 +             # residual connection
# 319 +             hidden_states = self.ln_2(residual + self.mlp(hidden_states))
# 320 +         elif NORM == "rms":
# 321 +             residual = hidden_states
# 322 +             hidden_states = self.ln_2(hidden_states)
# 323 +             feed_forward_hidden_states = self.mlp(hidden_states)
# 324 +             hidden_states = residual + feed_forward_hidden_states
# 325 +         else:
# 326 +             raise NotImplementedError
# 429 +         # debug(f'PAD_ID: {PAD_ID}')
# 334 -         return pro_allgen
# 334 ?                          -
# 514 +         return pro_allgen
# 335 -

