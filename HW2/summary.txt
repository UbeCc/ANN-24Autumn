########################
# Missing Files
########################
# .DS_Store

########################
# Additional Files
########################
# augmentation.py

########################
# Filled Code
########################
# ../codes/mlp/model.py:1
    def __init__(self, num_features, eps=1e-5, momentum=0.1):
        self.eps = eps
        self.momentum = momentum
        self.weight = nn.Parameter(torch.ones(num_features))
        self.bias = nn.Parameter(torch.zeros(num_features))
        # Buffers (not parameters)
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))
        # input: [batch_size, num_features]
        if self.training:
            batch_mean = input.mean(dim=0)
            batch_var = input.var(dim=0, unbiased=False)

            # with torch.no_grad():
            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean
            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var

            # Normalize
            x_normalized = (input - batch_mean) / torch.sqrt(batch_var + self.eps)
        else:
        # Use running mean and variance for inference
            x_normalized = (input - self.running_mean) / torch.sqrt(self.running_var + self.eps)

        # Scale and shift
        return self.weight * x_normalized + self.bias

# ../codes/mlp/model.py:2
        if self.training:
            # During training, scale the output by 1 / (1 - p) with **remaining** network
            mask = torch.rand(input.shape)
            mask = torch.where(mask > self.p, torch.tensor(1.0), torch.tensor(0.0))
            mask = mask.float().to("cuda:1")
            return mask * input * (1.0 / (1 - self.p))
        else:
            # During testing
            return input

# ../codes/mlp/model.py:3
        self.linear1 = nn.Linear(3072, hidden_size)
        self.bn1 = BatchNorm1d(hidden_size)
        self.relu = nn.ReLU()
        self.dropout = Dropout(drop_rate)
        self.linear2 = nn.Linear(hidden_size, 10)

# ../codes/mlp/model.py:4
        x = x.view(x.size(0), -1)
        logits = self.linear1(x)
        logits = self.bn1(logits)
        logits = self.relu(logits)
        logits = self.dropout(logits)
        logits = self.linear2(logits)

# ../codes/cnn/model.py:1
    # Reference: https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html
    # Reference: https://blog.sailor.plus/deep-learning/optimization/
    def __init__(self, num_features, eps=1e-5, momentum=0.1):
        self.eps = eps
        self.momentum = momentum
        self.weight = nn.Parameter(torch.ones(num_features))
        self.bias = nn.Parameter(torch.zeros(num_features))
        # Buffers
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))
        # input: [batch_size, num_features, height, width]
        if self.training:
            batch_mean = input.mean([0, 2, 3])
            batch_var = input.var([0, 2, 3], unbiased=False)
            with torch.no_grad():
                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean
                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var
            x_normalized = (input - batch_mean[None, :, None, None]) / torch.sqrt(batch_var[None, :, None, None] + self.eps)
        else:
            x_normalized = (input - self.running_mean[None, :, None, None]) / torch.sqrt(self.running_var[None, :, None, None] + self.eps)
        return self.weight[None, :, None, None] * x_normalized + self.bias[None, :, None, None]

# ../codes/cnn/model.py:2
        # input: [batch_size, num_feature_map * height * width]
        if self.training:
            # During training, scale the output by 1 / (1 - p) with **remaining** network
            mask = torch.rand(input.shape)
            mask = torch.where(mask > self.p, torch.tensor(1.0), torch.tensor(0.0))
            mask = mask.float().to(device)
            return mask * input * (1.0 / (1 - self.p))
        else:
            # During testing
            return input

# ../codes/cnn/model.py:3
        # input - Conv - BN - ReLU - Dropout - MaxPool - Conv - BN - ReLU - Dropout - MaxPool - Linear - loss
        self.conv1 = nn.Conv2d(3, 16, 3, stride=1, padding=1)
        self.bn1 = BatchNorm2d(16)
        self.bn_torch = nn.BatchNorm2d(16)
        self.relu1 = nn.ReLU()
        self.dropout1 = Dropout(drop_rate)
        self.dropout_torch1 = nn.Dropout(drop_rate)
        self.pool1 = nn.MaxPool2d(2, stride=2)
        self.conv2 = nn.Conv2d(16, 32, 3, stride=1, padding=1)
        self.bn2 = BatchNorm2d(32)
        self.relu2 = nn.ReLU()
        self.dropout2 = Dropout(drop_rate)
        self.dropout_torch2 = nn.Dropout(drop_rate)
        self.pool2 = nn.MaxPool2d(2, stride=2)
        self.fc = nn.Linear(32 * 8 * 8, 10)

# ../codes/cnn/model.py:4
        logits = self.conv1(x)
        logits = self.bn1(logits)
        logits = self.relu1(logits)
        logits = self.dropout1(logits)
        logits = self.pool1(logits)
        logits = self.conv2(logits)
        logits = self.bn2(logits)
        logits = self.relu2(logits)
        logits = self.dropout2(logits)
        logits = self.pool2(logits)
        logits = logits.view(logits.size(0), -1)
        logits = self.fc(logits)


########################
# References
########################
# https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html
# https://blog.sailor.plus/deep-learning/optimization/

########################
# Other Modifications
########################
# _codes/mlp/model.py -> ../codes/mlp/model.py
# 7 + from torch.nn import functional as F
# 8 +
# 45 + # In this code, we implement dropout in an alternative way. During the training process, we scale the
# 46 + # remaining network nodes' output by 1/(1-p),. At testing time, we do nothing in the dropout layer. It's
# 47 + # easy to find that this method has similar results to original dropout
# 67 + # model architecture: input -- Linear - BN - ReLU - Dropout - Linear - loss
# 40 -     def __init__(self, drop_rate=0.5):
# 69 +     def __init__(self, drop_rate=0.5, hidden_size=128):
# 69 ?                                     +++++++++++++++++
# 71 +         # we train on cifar-10 dataset, the input shape is bsize * 3072
# 60 -         return loss, acc
# 60 ?                         -
# 100 +         return loss, acc
# _codes/mlp/main.py -> ../codes/mlp/main.py
# 6 -
# 6 + import wandb
# 15 + augmentation = True
# 16 +
# 17 + use_wandb = True
# 18 + # use_wandb = False
# 19 +
# 20 + if use_wandb:
# 21 +     wandb.init(
# 22 +         project="ANN-HW2",
# 23 +         # name="mlp",
# 24 +   		# name="mlp-aug-self",
# 25 +         name="mlp-aug",
# 26 +   		# name="mlp-wobn",
# 27 +     	# name="mlp-l-r-d-b-l",
# 28 +     )
# 29 +
# 19 - parser.add_argument('--num_epochs', type=int, default=20,
# 19 ?                                                       ^
# 34 + parser.add_argument('--num_epochs', type=int, default=30,
# 34 ?                                                       ^
# 20 -     help='Number of training epoch. Default: 20')
# 20 ?                                              ^
# 35 +     help='Number of training epoch. Default: 30')
# 35 ?                                              ^
# 23 - parser.add_argument('--drop_rate', type=float, default=0.5,
# 38 + parser.add_argument('--drop_rate', type=float, default=0.25,
# 38 ?                                                          +
# 24 -     help='Drop rate of the Dropout Layer. Default: 0.5')
# 39 +     help='Drop rate of the Dropout Layer. Default: 0.25')
# 39 ?                                                      +
# 35 -
# 87 +
# 88 +     # print('train', st, ed)
# 89 +     if use_wandb:
# 90 +         wandb.log({"train_loss": loss_.cpu().data.numpy(), "train_acc": acc_.cpu().data.numpy()})
# 91 +
# 76 - def valid_epoch(model, X, y): # Valid Process
# 95 + def valid_epoch(model, X, y, test): # Valid Process
# 95 ?                            ++++++
# 86 -
# 105 +
# 110 +
# 111 +     # print('valid', st, ed, test)
# 112 +     if use_wandb:
# 113 +         if test:
# 114 +             wandb.log({"test_loss": loss_.cpu().data.numpy(), "test_acc": acc_.cpu().data.numpy()})
# 115 +         else:
# 116 +             wandb.log({"valid_loss": loss_.cpu().data.numpy(), "valid_acc": acc_.cpu().data.numpy()})
# 101 -     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# 127 +     device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")
# 127 ?                                ++
# 134 +
# 135 +         if augmentation:
# 136 +             X_val, y_val = X_train[20000:], y_train[20000:]
# 137 +             X_train, y_train = X_train[:20000], y_train[:20000]
# 138 +
# 139 +             X_train_aug, _, y_train_aug, _ = load_cifar_2d('../augmented_cifar-10_data')
# 140 +             # X_train_aug, _, y_train_aug, _ = load_cifar_2d('../cifar-10_data')
# 141 +
# 142 +             X_val_aug, y_val_aug = X_train_aug[20000:], y_train_aug[20000:]
# 143 +             X_train_aug, y_train_aug = X_train_aug[:20000], y_train_aug[:20000]
# 144 +
# 145 +             X_train = np.concatenate((X_train, X_train_aug), axis=0)
# 146 +             y_train = np.concatenate((y_train, y_train_aug), axis=0)
# 147 +
# 124 -             val_acc, val_loss = valid_epoch(mlp_model, X_val, y_val)
# 164 +             val_acc, val_loss = valid_epoch(mlp_model, X_val, y_val, test=False)
# 164 ?                                                                    ++++++++++++
# 129 -                 test_acc, test_loss = valid_epoch(mlp_model, X_test, y_test)
# 169 +                 test_acc, test_loss = valid_epoch(mlp_model, X_test, y_test, test=True)
# 169 ?                                                                            +++++++++++
# 200 +
# 201 +         if augmentation:
# 202 +             X_train_aug, _, y_train_aug, _ = load_cifar_2d('./cifar-10_data')
# 203 +             X_train = np.concatenate((X_train, X_train_aug), axis=0)
# 204 +             y_train = np.concatenate((y_train, y_train_aug), axis=0)
# _codes/cnn/model.py -> ../codes/cnn/model.py
# 7 +
# 8 + device = "cuda:3"
# 9 +
# 27 -
# 41 +
# 60 -         return loss, acc
# 60 ?                         -
# 107 +         return loss, acc
# _codes/cnn/main.py -> ../codes/cnn/main.py
# 11 + import wandb
# 12 + from model import Model
# 13 + from load_data import load_cifar_4d
# 14 + torch.backends.cudnn.enabled = False
# 12 - from model import Model
# 13 - from load_data import load_cifar_2d
# 14 -
# 16 + use_wandb = True
# 17 + if use_wandb:
# 18 +     wandb.init(
# 19 +         project="ANN-HW2",
# 20 +     	# name="cnn",
# 21 +      	name="cnn-b-d-r",
# 22 +      	# name="cnn-wodropout",
# 23 +     )
# 24 +
# 25 + device = "cuda:3"
# 16 -
# 21 - parser.add_argument('--learning_rate', type=float, default=1e-3,
# 21 ?                                                               ^
# 31 + parser.add_argument('--learning_rate', type=float, default=1e-2,
# 31 ?                                                               ^
# 22 -     help='Learning rate during optimization. Default: 1e-3')
# 22 ?                                                       ^  ^
# 32 +     help='Learning rate during optimization. Default: 5e-4')
# 32 ?                                                       ^  ^
# 23 - parser.add_argument('--drop_rate', type=float, default=0.5,
# 23 ?                                                          ^
# 33 + parser.add_argument('--drop_rate', type=float, default=0.2,
# 33 ?                                                          ^
# 24 -     help='Drop rate of the Dropout Layer. Default: 0.5')
# 24 ?                                                      ^
# 34 +     help='Drop rate of the Dropout Layer. Default: 0.2')
# 34 ?                                                      ^
# 83 +
# 84 +     if use_wandb:
# 85 +         wandb.log({"train_loss": loss_.cpu().data.numpy(), "train_acc": acc_.cpu().data.numpy()})
# 86 +
# 76 - def valid_epoch(model, X, y): # Valid Process
# 90 + def valid_epoch(model, X, y, test): # Valid Process
# 90 ?                            ++++++
# 105 +
# 106 +     if use_wandb:
# 107 +         if test:
# 108 +             wandb.log({"test_loss": loss_.cpu().data.numpy(), "test_acc": acc_.cpu().data.numpy()})
# 109 +         else:
# 110 +             wandb.log({"valid_loss": loss_.cpu().data.numpy(), "valid_acc": acc_.cpu().data.numpy()})
# 111 +
# 101 -     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# 101 ?                           ^ ^^^^
# 122 +     device = torch.device(device if torch.cuda.is_available() else "cpu")
# 122 ?                           ^^^^ ^
# 105 -         X_train, X_test, y_train, y_test = load_cifar_2d(args.data_dir)
# 105 ?                                                       ^
# 126 +         X_train, X_test, y_train, y_test = load_cifar_4d(args.data_dir)
# 126 ?                                                       ^
# 108 -         mlp_model = Model(drop_rate=drop_rate)
# 129 +         mlp_model = Model(drop_rate=args.drop_rate)
# 129 ?                                     +++++
# 124 -             val_acc, val_loss = valid_epoch(mlp_model, X_val, y_val)
# 145 +             val_acc, val_loss = valid_epoch(mlp_model, X_val, y_val, test=False)
# 145 ?                                                                    ++++++++++++
# 129 -                 test_acc, test_loss = valid_epoch(mlp_model, X_test, y_test)
# 150 +                 test_acc, test_loss = valid_epoch(mlp_model, X_test, y_test, test=True)
# 150 ?                                                                            +++++++++++
# 160 -         X_train, X_test, y_train, y_test = load_cifar_2d(args.data_dir)
# 160 ?                                                       ^
# 181 +         X_train, X_test, y_train, y_test = load_cifar_4d(args.data_dir)
# 181 ?                                                       ^

